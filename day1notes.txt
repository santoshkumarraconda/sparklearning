Apache Spark is considered as a replacement for the Map Reduce Engine of Hadoop.

Spark is considered to be 20 times faster than Hadoop's Map Reduce Engine.

Spark is used for large scale data processing. It is not necessary that it should be used only with Hadoop.

Scala is a functional programming language which runs on top of JVM.

JVM--------------Java Virtual Machine.

JRE-------------Java Runtime Environment---------------JVM+System Libraries.


JVM--------------Interprets the java byte code into runtime instructions.

Interactive Shell

	in scala--------spark shell
	in python------pyspark


RDD is the fundamental unit of data in spark.

RDD---Resilient Distributed Dataset


Spark Setup :

1. Download Jdk 8 from https://www.oracle.com/in/java/technologies/javase/javase8-archive-downloads.html#license-lightbox and install it.

2. set the JAVA_HOME environment variable.

3. Download spark from https://www.apache.org/dyn/closer.lua/spark/spark-3.1.3/spark-3.1.3-bin-hadoop3.2.tgz and extract it to c:\.

4. set the SPARK_HOME environment variable.

5. Spark uses some of shell scripts from Hadoop. Since we don't have Hadoop installed in the system, we need to download some of hadoop 
scrips for windows so that spark runs without Hadoop.

Download winutils for hadoop from https://github.com/steveloughran/winutils.git and extract it to c:\.

6. set the HADOOP_HOME environment variable.

7. Add %JAVA_HOME%\bin,%SPARK_HOME%\bin and %HADOOP_HOME%\bin to the path.

8. To verify the installation, open a new command and run the command spark-shell to open the interactive shell in scala.


textFile function defined in an object called sc.
sc-----------------implicit SparkContext object.

SparkContext is the entry point to spark cluster.
The developer interacts with the spark cluster through the SparkContext object.
An implicit SparkContext is already available in spark-shell  and it can be accessed throught sc  variable.


conf.setMaster("local[*]")

-----------uses the local machine for the cluster. * denotes that it used as many number of workers(slaves) as the number of core processors in the 
local machine.

Transformation---------------Intermediate operation-------------------returns another RDD
Action---------------------------Terminal operation-------------------------returns values

Data in the rdd are divided across multiple partitions and stored.


A partition in spark is an atomic chunk of data(logical division of data) stored on a node in the cluster.
Paritions are basic units of parallelism in Apache Spark. RDDs  are collection of partitions.


Each of the transformation functions take a function as an argument and returns another rdd.

class RDD[T]{
	def map(f: T=>T1): RDD[T1]
}


class JavaRDD<T>{
	JavaRDD<T1> map(f: T->T1)
}

/*@FunctionalInterface
interface intf1<T,T1>{
	T1 apply(T t)
}*/


class RDD[T]{
	def  filter(f: T=>boolean): RDD[T]// The resultant RDD will have only the elements  for which parameter function returned true.
}


flatMap():

The flatMap() is equivalent to map() and flatten() functions.

The parameter to flatMap is a function which accepts an element as  argument and returns an array.
Since each element returns an array, the result will be an array of array and then flatten function is applied on this 2 dimensional array to make it one dimensional.


Pair RDD:

It is an RDD of two element tuples.

A tuple is value which represents multiple elements. A tuple is most commonly used to return multiple values from a function.



Elements of the RDD----------------("a",5),("b",4),("c",8),("a",3),("d",7),("b",4),("a",7),("b",3),("b",1)
reduceByKey((x,y)=>x+y)

for key "a" it performs the  following steps

(5,3)=>8
(8,7)=>15

for key "b"

(4,4)=>8
(8,3)=>11
(11,1)=>12

for key "c"=>8

for ke "d"=>7










