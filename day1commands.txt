spark-shell

val rdd1=sc.textFile("c:/test/sample.txt")

rdd1.collect()

scala> val rdd1=sc.textFile("c:/test")
rdd1: org.apache.spark.rdd.RDD[String] = c:/test MapPartitionsRDD[3] at textFile at <console>:24

scala> rdd1.collect()


<dependencies>
  <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.12</artifactId>
    <version>3.0.3</version>
</dependency>
  
  </dependencies>


project: SparkCorePrj
package: com.jpmc.training.sparkcore
class: RDDCreationTest1



package com.jpmc.training.sparkcore;

import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
RDDCreationTest
public class 1 {
	public static void main(String[] args) {
		SparkConf conf=new SparkConf();
		conf.setMaster("local[*]");
		conf.setAppName("rdd-create-app");
		JavaSparkContext sc=new JavaSparkContext(conf);
		
		JavaRDD<String> rdd1=sc.textFile("c:/test/sample.txt");
		List<String> list= rdd1.collect();
		list.forEach(System.out::println);
	}

}




scala> val array=Array(10,40,60,12,45)


scala> val rdd2=sc.parallelize(array)
rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at <console>:26

scala> rdd2.collect()


scala> rdd2.collect().foreach(println)



package com.jpmc.training.sparkcore;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class RDDCreationTest2 {
	public static void main(String[] args) {
		SparkConf conf=new SparkConf();
		conf.setMaster("local[*]");
		conf.setAppName("rdd-create-app");
		JavaSparkContext sc=new JavaSparkContext(conf);
		List<Integer> list=Arrays.asList(12,55,22,8,10,28);
		
		JavaRDD<Integer> rdd1=sc.parallelize(list);
		rdd1.collect().forEach(System.out::println);
	}

}


scala> rdd2.take(3)
res9: Array[Int] = Array(10, 40, 60)

scala> rdd1.collect()
res10: Array[String] = Array(This is a test file, Spark is fun, Scala is interesting)

scala> rdd1.getNumPartitions
res11: Int = 2

scala> rdd1.collect
res12: Array[String] = Array(This is a test file, Spark is fun, Scala is interesting)

scala> rdd1.saveAsTextFile("c:/testout")

scala> val rdd1=sc.textFile("c:/test/sample.txt",4)
rdd1: org.apache.spark.rdd.RDD[String] = c:/test/sample.txt MapPartitionsRDD[9] at textFile at <console>:24

scala> rdd1.getNumPartitions


scala> val array=Array(10,40,60,12,45)
array: Array[Int] = Array(10, 40, 60, 12, 45)

scala> val rdd2=sc.parallelize(array)
rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at <console>:26

scala> val rdd3=rdd2.map(n=>n*2)
rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[11] at map at <console>:25

scala> rdd2.collect
res15: Array[Int] = Array(10, 40, 60, 12, 45)

scala> rdd3.collect
res16: Array[Int] = Array(20, 80, 120, 24, 90)'



scala> val rdd4=rdd2.map(n=>"value is "+n)
rdd4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[12] at map at <console>:25

scala> rdd4.collect

scala> val rdd3=rdd2.filter(n=>n>30)
rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[13] at filter at <console>:25

scala> rdd3.collect

scala> rdd2.map(a=>a*5).filter(a=>a<100).collect



package com.jpmc.training.sparkcore;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class TransformationTest {
	public static void main(String[] args) {
		SparkConf conf=new SparkConf();
		conf.setMaster("local[*]");
		conf.setAppName("transformation-test-app");
		JavaSparkContext sc=new JavaSparkContext(conf);
		List<Integer> list=Arrays.asList(12,55,22,8,10,28);
		
		JavaRDD<Integer> rdd1=sc.parallelize(list);
		rdd1.map(n->n*5).filter(n->n>=100).collect().forEach(System.out::println);
	}

}



scala> var arr=Array(2,6,10,7)
arr: Array[Int] = Array(2, 6, 10, 7)

scala> val rdd=sc.parallelize(arr)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[16] at parallelize at <console>:26

scala> rdd.map(n=>Array(n-1,n,n+1)).collect
res22: Array[Array[Int]] = Array(Array(1, 2, 3), Array(5, 6, 7), Array(9, 10, 11), Array(6, 7, 8))

scala> rdd.flatMap(n=>Array(n-1,n,n+1)).collect
res23: Array[Int] = Array(1, 2, 3, 5, 6, 7, 9, 10, 11, 6, 7, 8)

scala> val rdd1=rdd.map(n=>Array(n-1,n,n+1))
rdd1: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[19] at map at <console>:25

scala> val rdd2=rdd.flatMap(n=>Array(n-1,n,n+1))
rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[20] at flatMap at <console>:25

scala> rdd1.collect
res24: Array[Array[Int]] = Array(Array(1, 2, 3), Array(5, 6, 7), Array(9, 10, 11), Array(6, 7, 8))

scala> rdd2.collect
res25: Array[Int] = Array(1, 2, 3, 5, 6, 7, 9, 10, 11, 6, 7, 8)


package com.jpmc.training.sparkcore;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

public class FlatMapTest {
	public static void main(String[] args) {
		SparkConf conf=new SparkConf();
		conf.setMaster("local[*]");
		conf.setAppName("transformation-test-app");
		JavaSparkContext sc=new JavaSparkContext(conf);
		
		
		JavaRDD<String> rdd1=sc.textFile("c:/test/sample.txt");
		JavaRDD<String> wordsRdd=rdd1.flatMap(line->Arrays.asList(line.split(" ")).iterator());
		wordsRdd.collect().forEach(System.out::println);
				
	}

}


 sc.textFile("c:/test/sample.txt").flatMap(line=>line.split(" ")).collect()


sc.setLogLevel("WARN");



scala> val t=(23,55,12.444,"hello")
t: (Int, Int, Double, String) = (23,55,12.444,hello)

scala> t._1
res27: Int = 23

scala> t._2
res28: Int = 55

scala> t._4



scala> val rdd1=sc.parallelize(Array(("a",2),("b",3),("a",6),("c",10),("b",3)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[24] at parallelize at <console>:24

scala> val rdd2=rdd1.groupByKey()
rdd2: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[25] at groupByKey at <console>:25

scala> rdd2.collect()


val rdd2=rdd1.countByKey()


scala> val rdd1=sc.parallelize(Array(("a",2),("b",4),("a",6),("c",10),("b",3)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[28] at parallelize at <console>:24

scala> val rdd2=rdd1.reduceByKey((x,y)=>x+y)
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[29] at reduceByKey at <console>:25

scala> rdd2.collect


scala> val rdd1=sc.parallelize(Array(("a",5),("b",4),("c",8),("a",3),("d",7),("b",4),("a",7),("b",3),("b",1)))
rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[30] at parallelize at <console>:24

scala> val rdd2=rdd1.reduceByKey((x,y)=>x+y)
rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[31] at reduceByKey at <console>:25

scala> rdd2.collect()



package com.jpmc.training.sparkcore;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

import scala.Tuple2;

public class PairRddTest {
	public static void main(String[] args) {
		SparkConf conf=new SparkConf();
		conf.setMaster("local[*]");
		conf.setAppName("transformation-test-app");
		JavaSparkContext sc=new JavaSparkContext(conf);
		sc.setLogLevel("WARN");
		
		List<Tuple2<String, Integer>> list=Arrays.asList(new Tuple2<>("a", 2),new Tuple2<>("a", 2),
				new Tuple2<>("b", 3),new Tuple2<>("a", 3));
		JavaPairRDD<String,Integer> rdd1=sc.parallelizePairs(list);
		rdd1.reduceByKey((x,y)->x+y).collect().forEach(System.out::println);
		
				
	}

}



https://drive.google.com/drive/folders/1wAiJCMDJ5jgIXAraR65gLtIYh0SOuVoW?usp=sharing

